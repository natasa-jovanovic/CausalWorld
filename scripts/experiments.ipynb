{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'CurriculumInterventionActorPolicy' from 'causal_world.intervention_actors' (c:\\users\\psiml8\\causalworld\\causal_world\\intervention_actors\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10792\\1472755085.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcausal_world\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEvaluationPipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mcausal_world\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintervention_actors\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomInterventionActorPolicy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGoalInterventionActorPolicy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCurriculumInterventionActorPolicy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcausal_world\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbenchmark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbenchmarks\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPUSHING_BENCHMARK\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPICKING_BENCHMARK\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mPICK_AND_PLACE_BENCHMARK\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSTACKING2_BENCHMARK\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcausal_world\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtask_generators\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtask\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgenerate_task\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'CurriculumInterventionActorPolicy' from 'causal_world.intervention_actors' (c:\\users\\psiml8\\causalworld\\causal_world\\intervention_actors\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from causal_world.evaluation.evaluation import EvaluationPipeline\n",
    "from causal_world.intervention_actors import RandomInterventionActorPolicy, GoalInterventionActorPolicy, CurriculumInterventionActorPolicy\n",
    "from causal_world.benchmark.benchmarks import PUSHING_BENCHMARK, PICKING_BENCHMARK, \\\n",
    "    PICK_AND_PLACE_BENCHMARK, STACKING2_BENCHMARK\n",
    "from causal_world.task_generators.task import generate_task\n",
    "import causal_world.viewers.task_viewer as viewer\n",
    "import causal_world.evaluation.visualization.visualiser as vis\n",
    "\n",
    "from scripts.util import utils as utils, utils_baselines as utils_baselines\n",
    "import argparse\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0865a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2978c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6668525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model(model_num, task):\n",
    "    if task == 'pushing':\n",
    "        benchmarks = utils.sweep('benchmarks', [PUSHING_BENCHMARK])\n",
    "        task_configs = [{\n",
    "            'task_configs': {\n",
    "                'variables_space': 'space_a',\n",
    "                'fractional_reward_weight': 1,\n",
    "                'dense_reward_weights': [750, 250, 0]\n",
    "            }\n",
    "        }]\n",
    "    elif task == 'picking':\n",
    "        benchmarks = utils.sweep('benchmarks', [PICKING_BENCHMARK])\n",
    "        task_configs = [{\n",
    "            'task_configs': {\n",
    "                'variables_space': 'space_a',\n",
    "                'fractional_reward_weight': 1,\n",
    "                'dense_reward_weights': [250, 0, 125,\n",
    "                                         0, 750, 0,\n",
    "                                         0, 0.005]\n",
    "            }\n",
    "        }]\n",
    "    elif task == 'pick_and_place':\n",
    "        benchmarks = utils.sweep('benchmarks', [PICK_AND_PLACE_BENCHMARK])\n",
    "        task_configs = [{\n",
    "            'task_configs': {\n",
    "                'variables_space': 'space_a',\n",
    "                'fractional_reward_weight': 1,\n",
    "                'dense_reward_weights': [750, 50, 250, 0, 0.005]\n",
    "            }\n",
    "        }]\n",
    "    elif task == 'stacking2':\n",
    "        benchmarks = utils.sweep('benchmarks', [STACKING2_BENCHMARK])\n",
    "        task_configs = [{\n",
    "            'task_configs': {\n",
    "                'variables_space': 'space_a',\n",
    "                'fractional_reward_weight': 1,\n",
    "                'dense_reward_weights': [750, 250,\n",
    "                                         250, 125,\n",
    "                                         0.005]\n",
    "            }\n",
    "        }]\n",
    "    else:\n",
    "        benchmarks = utils.sweep('benchmarks', [PUSHING_BENCHMARK])\n",
    "        task_configs = [{\n",
    "            'task_configs': {\n",
    "                'variables_space': 'space_a',\n",
    "                'fractional_reward_weight': 1,\n",
    "                'dense_reward_weights': [750, 250, 0]\n",
    "            }\n",
    "        }]\n",
    "\n",
    "    world_params = [{\n",
    "        'world_params': {\n",
    "            'skip_frame': 3,\n",
    "            'enable_visualization': False,\n",
    "            'observation_mode': 'structured',\n",
    "            'normalize_observations': True,\n",
    "            'action_mode': 'joint_positions'\n",
    "        }\n",
    "    }]\n",
    "\n",
    "    net_layers = utils.sweep('NET_LAYERS', [[256, 256]])\n",
    "    world_seed = utils.sweep('world_seed', [0])\n",
    "    NUM_RANDOM_SEEDS = 5\n",
    "    random_seeds = utils.sweep('seed', list(range(NUM_RANDOM_SEEDS)))\n",
    "\n",
    "    ppo = {'num_of_envs': 20,\n",
    "           'algorithm': 'PPO',\n",
    "           'validate_every_timesteps': int(20000),\n",
    "           'total_time_steps': int(10000000),\n",
    "           'train_configs': {\n",
    "               \"gamma\": 0.99,\n",
    "               \"n_steps\": int(1200 / 20),\n",
    "               \"ent_coef\": 0.01,\n",
    "               \"learning_rate\": 0.00025,\n",
    "               \"vf_coef\": 0.5,\n",
    "               \"max_grad_norm\": 0.5,\n",
    "               \"nminibatches\": 40,\n",
    "               \"noptepochs\": 4\n",
    "           }}\n",
    "\n",
    "    sac = {'num_of_envs': 1,\n",
    "           'algorithm': 'SAC',\n",
    "           'validate_every_timesteps': int(500000),\n",
    "           'total_time_steps': int(1e7),\n",
    "           'train_configs': {\n",
    "               \"gamma\": 0.95,\n",
    "               \"tau\": 1e-3,\n",
    "               \"ent_coef\": 1e-3,\n",
    "               \"target_entropy\": 'auto',\n",
    "               \"learning_rate\":  1e-4,\n",
    "               \"buffer_size\": 1000000,\n",
    "               \"learning_starts\": 1000,\n",
    "               \"batch_size\": 256\n",
    "           }}\n",
    "\n",
    "    td3 = {'num_of_envs': 1,\n",
    "           'algorithm': 'TD3',\n",
    "           'validate_every_timesteps': int(500000),\n",
    "           'total_time_steps': int(10000000),\n",
    "           'train_configs': {\n",
    "               \"gamma\": 0.96,\n",
    "               \"tau\": 0.02,\n",
    "               \"learning_rate\": 1e-4,\n",
    "               \"buffer_size\": 500000,\n",
    "               \"learning_starts\": 1000,\n",
    "               \"batch_size\": 128}}\n",
    "\n",
    "    algorithms = [ppo, sac]\n",
    "\n",
    "    curriculum_kwargs_1 = {'intervention_actors': [], 'actives': []}\n",
    "    curriculum_kwargs_2 = {\n",
    "        'intervention_actors': [GoalInterventionActorPolicy()],\n",
    "        'actives': [(0, 1e9, 1, 0)]\n",
    "    }\n",
    "    curriculum_kwargs_3 = {\n",
    "        'intervention_actors': [RandomInterventionActorPolicy()],\n",
    "        'actives': [(0, 1e9, 1, 0)]\n",
    "    }\n",
    "    curriculum_kwargs_4 = {\n",
    "        'intervention_actors': [Cur()],\n",
    "        'actives': [(0, 1e7, 7, 0)]\n",
    "    }\n",
    "    curriculum_kwargs = [\n",
    "        curriculum_kwargs_1, curriculum_kwargs_2, curriculum_kwargs_3\n",
    "    ]\n",
    "\n",
    "    return utils.outer_product([\n",
    "        benchmarks, world_params, task_configs, algorithms, curriculum_kwargs,\n",
    "        random_seeds, world_seed, net_layers\n",
    "    ])[model_num]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c2b715",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--model_num\",\n",
    "                    required=True,\n",
    "                    default=0,\n",
    "                    help=\"model number\")\n",
    "parser.add_argument(\"--task\",\n",
    "                    required=True,\n",
    "                    default='pushing',\n",
    "                    help=\"possible tasks: pushing, picking, pick_and_place, stacking2\")\n",
    "parser.add_argument(\"--output_path\", required=True, help=\"output path\")\n",
    "# parser.add_argument('--tensorboard', help=\"tensorboard logging\")\n",
    "\n",
    "tensorboard_logging = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc01ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_num = 1\n",
    "task = \"pushing\"\n",
    "output_path = \"experiments-pushing\"\n",
    "output_path = os.path.join(output_path, str(model_num))\n",
    "try:\n",
    "    os.makedirs(output_path)\n",
    "except FileExistsError:\n",
    "    print(\"Folder '{}' already exists. Will try to load existing checkpoints\".format(output_path))\n",
    "\n",
    "model_settings = baseline_model(model_num, task)\n",
    "\n",
    "model = utils_baselines.train_model(model_settings, output_path, tensorboard_logging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0be6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_fn(obs, prev_action=None, prev_reward=None):\n",
    "        return model.predict(obs, deterministic=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d305b446",
   "metadata": {},
   "outputs": [],
   "source": [
    "animation_path = os.path.join(output_path, 'animation')\n",
    "os.makedirs(animation_path)\n",
    "viewer.record_video_of_policy(task=generate_task(\n",
    "    task_generator_id=model_settings['benchmarks']['task_generator_id'],\n",
    "    **model_settings['task_configs']),\n",
    "    world_params=model_settings['world_params'],\n",
    "    policy_fn=policy_fn,\n",
    "    file_name=os.path.join(\n",
    "        animation_path, \"policy\"),\n",
    "    number_of_resets=1,\n",
    "    max_time_steps=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32dd900",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_path = os.path.join(output_path, 'evaluation',\n",
    "                                   'time_steps_{}'.format(model_settings['total_time_steps']))\n",
    "os.makedirs(evaluation_path)\n",
    "\n",
    "evaluation_protocols = model_settings['benchmarks']['evaluation_protocols']\n",
    "\n",
    "evaluator = EvaluationPipeline(evaluation_protocols=evaluation_protocols,\n",
    "                                tracker_path=output_path,\n",
    "                                initial_seed=0)\n",
    "scores = evaluator.evaluate_policy(policy_fn, fraction=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9353a7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_model = dict()\n",
    "results_model.update(scores)\n",
    "results_model.update(model_settings)\n",
    "results_model.update({'model_num': model_num})\n",
    "del results_model['benchmarks']\n",
    "file_name_results_model = os.path.join(evaluation_path, 'results_model.json')\n",
    "utils.save_model_settings(file_name_results_model, results_model)\n",
    "evaluator.save_scores(evaluation_path)\n",
    "experiments = dict()\n",
    "experiments[str(model_num)] = scores\n",
    "vis.generate_visual_analysis(evaluation_path, experiments=experiments)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 ('causal_world')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "14dc9ff527c7255c22ab1b01300b8a4e43cdfe49563da012c0b196015f955bf1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
