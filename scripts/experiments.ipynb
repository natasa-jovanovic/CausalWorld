{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\psiml8\\Anaconda3\\envs\\causal_world\\lib\\site-packages\\stable_baselines\\__init__.py:33: UserWarning: stable-baselines is in maintenance mode, please use [Stable-Baselines3 (SB3)](https://github.com/DLR-RM/stable-baselines3) for an up-to-date version. You can find a [migration guide](https://stable-baselines3.readthedocs.io/en/master/guide/migration.html) in SB3 documentation.\n",
      "  \"stable-baselines is in maintenance mode, please use [Stable-Baselines3 (SB3)](https://github.com/DLR-RM/stable-baselines3) for an up-to-date version. You can find a [migration guide](https://stable-baselines3.readthedocs.io/en/master/guide/migration.html) in SB3 documentation.\"\n"
     ]
    }
   ],
   "source": [
    "from causal_world.evaluation.evaluation import EvaluationPipeline\n",
    "from causal_world.intervention_actors import RandomInterventionActorPolicy, GoalInterventionActorPolicy\n",
    "from causal_world.benchmark.benchmarks import PUSHING_BENCHMARK, PICKING_BENCHMARK, \\\n",
    "    PICK_AND_PLACE_BENCHMARK, STACKING2_BENCHMARK\n",
    "from causal_world.task_generators.task import generate_task\n",
    "import causal_world.viewers.task_viewer as viewer\n",
    "import causal_world.evaluation.visualization.visualiser as vis\n",
    "\n",
    "from util import utils as utils, utils_baselines as utils_baselines\n",
    "import argparse\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6668525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model(model_num, task):\n",
    "    if task == 'pushing':\n",
    "        benchmarks = utils.sweep('benchmarks', [PUSHING_BENCHMARK])\n",
    "        task_configs = [{\n",
    "            'task_configs': {\n",
    "                'variables_space': 'space_a',\n",
    "                'fractional_reward_weight': 1,\n",
    "                'dense_reward_weights': [750, 250, 0]\n",
    "            }\n",
    "        }]\n",
    "    elif task == 'picking':\n",
    "        benchmarks = utils.sweep('benchmarks', [PICKING_BENCHMARK])\n",
    "        task_configs = [{\n",
    "            'task_configs': {\n",
    "                'variables_space': 'space_a',\n",
    "                'fractional_reward_weight': 1,\n",
    "                'dense_reward_weights': [250, 0, 125,\n",
    "                                         0, 750, 0,\n",
    "                                         0, 0.005]\n",
    "            }\n",
    "        }]\n",
    "    elif task == 'pick_and_place':\n",
    "        benchmarks = utils.sweep('benchmarks', [PICK_AND_PLACE_BENCHMARK])\n",
    "        task_configs = [{\n",
    "            'task_configs': {\n",
    "                'variables_space': 'space_a',\n",
    "                'fractional_reward_weight': 1,\n",
    "                'dense_reward_weights': [750, 50, 250, 0, 0.005]\n",
    "            }\n",
    "        }]\n",
    "    elif task == 'stacking2':\n",
    "        benchmarks = utils.sweep('benchmarks', [STACKING2_BENCHMARK])\n",
    "        task_configs = [{\n",
    "            'task_configs': {\n",
    "                'variables_space': 'space_a',\n",
    "                'fractional_reward_weight': 1,\n",
    "                'dense_reward_weights': [750, 250,\n",
    "                                         250, 125,\n",
    "                                         0.005]\n",
    "            }\n",
    "        }]\n",
    "    else:\n",
    "        benchmarks = utils.sweep('benchmarks', [PUSHING_BENCHMARK])\n",
    "        task_configs = [{\n",
    "            'task_configs': {\n",
    "                'variables_space': 'space_a',\n",
    "                'fractional_reward_weight': 1,\n",
    "                'dense_reward_weights': [750, 250, 0]\n",
    "            }\n",
    "        }]\n",
    "\n",
    "    world_params = [{\n",
    "        'world_params': {\n",
    "            'skip_frame': 3,\n",
    "            'enable_visualization': False,\n",
    "            'observation_mode': 'structured',\n",
    "            'normalize_observations': True,\n",
    "            'action_mode': 'joint_positions'\n",
    "        }\n",
    "    }]\n",
    "\n",
    "    net_layers = utils.sweep('NET_LAYERS', [[256, 256]])\n",
    "    world_seed = utils.sweep('world_seed', [0])\n",
    "    NUM_RANDOM_SEEDS = 5\n",
    "    random_seeds = utils.sweep('seed', list(range(NUM_RANDOM_SEEDS)))\n",
    "\n",
    "    ppo = {'num_of_envs': 20,\n",
    "           'algorithm': 'PPO',\n",
    "           'validate_every_timesteps': int(20000),\n",
    "           'total_time_steps': int(10000000),\n",
    "           'train_configs': {\n",
    "               \"gamma\": 0.99,\n",
    "               \"n_steps\": int(1200 / 20),\n",
    "               \"ent_coef\": 0.01,\n",
    "               \"learning_rate\": 0.00025,\n",
    "               \"vf_coef\": 0.5,\n",
    "               \"max_grad_norm\": 0.5,\n",
    "               \"nminibatches\": 40,\n",
    "               \"noptepochs\": 4\n",
    "           }}\n",
    "\n",
    "    sac = {'num_of_envs': 1,\n",
    "           'algorithm': 'SAC',\n",
    "           'validate_every_timesteps': int(500000),\n",
    "           'total_time_steps': int(10000000),\n",
    "           'train_configs': {\n",
    "               \"gamma\": 0.95,\n",
    "               \"tau\": 1e-3,\n",
    "               \"ent_coef\": 1e-3,\n",
    "               \"target_entropy\": 'auto',\n",
    "               \"learning_rate\":  1e-4,\n",
    "               \"buffer_size\": 1000000,\n",
    "               \"learning_starts\": 1000,\n",
    "               \"batch_size\": 256\n",
    "           }}\n",
    "\n",
    "    td3 = {'num_of_envs': 1,\n",
    "           'algorithm': 'TD3',\n",
    "           'validate_every_timesteps': int(500000),\n",
    "           'total_time_steps': int(10000000),\n",
    "           'train_configs': {\n",
    "               \"gamma\": 0.96,\n",
    "               \"tau\": 0.02,\n",
    "               \"learning_rate\": 1e-4,\n",
    "               \"buffer_size\": 500000,\n",
    "               \"learning_starts\": 1000,\n",
    "               \"batch_size\": 128}}\n",
    "\n",
    "    algorithms = [ppo, sac]\n",
    "\n",
    "    curriculum_kwargs_1 = {'intervention_actors': [], 'actives': []}\n",
    "    curriculum_kwargs_2 = {\n",
    "        'intervention_actors': [GoalInterventionActorPolicy()],\n",
    "        'actives': [(0, 1e9, 1, 0)]\n",
    "    }\n",
    "    curriculum_kwargs_3 = {\n",
    "        'intervention_actors': [RandomInterventionActorPolicy()],\n",
    "        'actives': [(0, 1e9, 1, 0)]\n",
    "    }\n",
    "    curriculum_kwargs = [\n",
    "        curriculum_kwargs_1, curriculum_kwargs_2, curriculum_kwargs_3\n",
    "    ]\n",
    "\n",
    "    return utils.outer_product([\n",
    "        benchmarks, world_params, task_configs, algorithms, curriculum_kwargs,\n",
    "        random_seeds, world_seed, net_layers\n",
    "    ])[model_num]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71c2b715",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--model_num\",\n",
    "                    required=True,\n",
    "                    default=0,\n",
    "                    help=\"model number\")\n",
    "parser.add_argument(\"--task\",\n",
    "                    required=True,\n",
    "                    default='pushing',\n",
    "                    help=\"possible tasks: pushing, picking, pick_and_place, stacking2\")\n",
    "parser.add_argument(\"--output_path\", required=True, help=\"output path\")\n",
    "# parser.add_argument('--tensorboard', help=\"tensorboard logging\")\n",
    "\n",
    "tensorboard_logging = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fc01ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder 'experiments-pushing\\0' already exists. Will try to load existing checkpoints\n",
      "Folder 'experiments-pushing\\0\\model' already exists\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x000001F357EB92C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x000001F357EB92C8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x000001F357BF8DC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x000001F357BF8DC8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "-------------------------------------\n",
      "| approxkl           | 0.008247341  |\n",
      "| clipfrac           | 0.11430417   |\n",
      "| ep_len_mean        | 834          |\n",
      "| ep_reward_mean     | -0.769       |\n",
      "| explained_variance | -0.0653      |\n",
      "| fps                | 1274         |\n",
      "| n_updates          | 1            |\n",
      "| policy_entropy     | 12.793945    |\n",
      "| policy_loss        | -0.008667563 |\n",
      "| serial_timesteps   | 6000         |\n",
      "| time_elapsed       | 0            |\n",
      "| total_timesteps    | 120000       |\n",
      "| value_loss         | 0.06555984   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| approxkl           | 0.007918061  |\n",
      "| clipfrac           | 0.10838954   |\n",
      "| ep_len_mean        | 834          |\n",
      "| ep_reward_mean     | -0.553       |\n",
      "| explained_variance | 0.112        |\n",
      "| fps                | 1265         |\n",
      "| n_updates          | 2            |\n",
      "| policy_entropy     | 12.816803    |\n",
      "| policy_loss        | -0.009274585 |\n",
      "| serial_timesteps   | 12000        |\n",
      "| time_elapsed       | 94.2         |\n",
      "| total_timesteps    | 240000       |\n",
      "| value_loss         | 0.05208426   |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| approxkl           | 0.007598658 |\n",
      "| clipfrac           | 0.10104791  |\n",
      "| ep_len_mean        | 834         |\n",
      "| ep_reward_mean     | -0.277      |\n",
      "| explained_variance | 0.248       |\n",
      "| fps                | 1212        |\n",
      "| n_updates          | 3           |\n",
      "| policy_entropy     | 12.836647   |\n",
      "| policy_loss        | -0.00892329 |\n",
      "| serial_timesteps   | 18000       |\n",
      "| time_elapsed       | 189         |\n",
      "| total_timesteps    | 360000      |\n",
      "| value_loss         | 0.047737233 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| approxkl           | 0.008238134 |\n",
      "| clipfrac           | 0.11311877  |\n",
      "| ep_len_mean        | 834         |\n",
      "| ep_reward_mean     | -0.37       |\n",
      "| explained_variance | 0.351       |\n",
      "| fps                | 1252        |\n",
      "| n_updates          | 4           |\n",
      "| policy_entropy     | 12.852      |\n",
      "| policy_loss        | -0.00928585 |\n",
      "| serial_timesteps   | 24000       |\n",
      "| time_elapsed       | 288         |\n",
      "| total_timesteps    | 480000      |\n",
      "| value_loss         | 0.040639203 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| approxkl           | 0.008286666  |\n",
      "| clipfrac           | 0.11500001   |\n",
      "| ep_len_mean        | 834          |\n",
      "| ep_reward_mean     | 0.000766     |\n",
      "| explained_variance | 0.413        |\n",
      "| fps                | 1221         |\n",
      "| n_updates          | 5            |\n",
      "| policy_entropy     | 12.864435    |\n",
      "| policy_loss        | -0.009736554 |\n",
      "| serial_timesteps   | 30000        |\n",
      "| time_elapsed       | 384          |\n",
      "| total_timesteps    | 600000       |\n",
      "| value_loss         | 0.039079584  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| approxkl           | 0.008385101  |\n",
      "| clipfrac           | 0.114933334  |\n",
      "| ep_len_mean        | 834          |\n",
      "| ep_reward_mean     | 0.109        |\n",
      "| explained_variance | 0.466        |\n",
      "| fps                | 1235         |\n",
      "| n_updates          | 6            |\n",
      "| policy_entropy     | 12.868846    |\n",
      "| policy_loss        | -0.008274827 |\n",
      "| serial_timesteps   | 36000        |\n",
      "| time_elapsed       | 482          |\n",
      "| total_timesteps    | 720000       |\n",
      "| value_loss         | 0.036426716  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| approxkl           | 0.008205447  |\n",
      "| clipfrac           | 0.11301875   |\n",
      "| ep_len_mean        | 834          |\n",
      "| ep_reward_mean     | 0.281        |\n",
      "| explained_variance | 0.529        |\n",
      "| fps                | 1224         |\n",
      "| n_updates          | 7            |\n",
      "| policy_entropy     | 12.873899    |\n",
      "| policy_loss        | -0.008893408 |\n",
      "| serial_timesteps   | 42000        |\n",
      "| time_elapsed       | 579          |\n",
      "| total_timesteps    | 840000       |\n",
      "| value_loss         | 0.032135766  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| approxkl           | 0.008217257  |\n",
      "| clipfrac           | 0.110943735  |\n",
      "| ep_len_mean        | 834          |\n",
      "| ep_reward_mean     | 0.482        |\n",
      "| explained_variance | 0.583        |\n",
      "| fps                | 1255         |\n",
      "| n_updates          | 8            |\n",
      "| policy_entropy     | 12.876666    |\n",
      "| policy_loss        | -0.007423243 |\n",
      "| serial_timesteps   | 48000        |\n",
      "| time_elapsed       | 677          |\n",
      "| total_timesteps    | 960000       |\n",
      "| value_loss         | 0.02782007   |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| approxkl           | 0.00804281    |\n",
      "| clipfrac           | 0.10807917    |\n",
      "| ep_len_mean        | 834           |\n",
      "| ep_reward_mean     | 0.602         |\n",
      "| explained_variance | 0.57          |\n",
      "| fps                | 1090          |\n",
      "| n_updates          | 9             |\n",
      "| policy_entropy     | 12.871076     |\n",
      "| policy_loss        | -0.0075396174 |\n",
      "| serial_timesteps   | 54000         |\n",
      "| time_elapsed       | 773           |\n",
      "| total_timesteps    | 1080000       |\n",
      "| value_loss         | 0.031217882   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| approxkl           | 0.008391625   |\n",
      "| clipfrac           | 0.114914596   |\n",
      "| ep_len_mean        | 834           |\n",
      "| ep_reward_mean     | 0.706         |\n",
      "| explained_variance | 0.597         |\n",
      "| fps                | 934           |\n",
      "| n_updates          | 10            |\n",
      "| policy_entropy     | 12.864107     |\n",
      "| policy_loss        | -0.0071428493 |\n",
      "| serial_timesteps   | 60000         |\n",
      "| time_elapsed       | 883           |\n",
      "| total_timesteps    | 1200000       |\n",
      "| value_loss         | 0.027195206   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| approxkl           | 0.007965779  |\n",
      "| clipfrac           | 0.10736877   |\n",
      "| ep_len_mean        | 834          |\n",
      "| ep_reward_mean     | 0.798        |\n",
      "| explained_variance | 0.62         |\n",
      "| fps                | 1020         |\n",
      "| n_updates          | 11           |\n",
      "| policy_entropy     | 12.850275    |\n",
      "| policy_loss        | -0.007290984 |\n",
      "| serial_timesteps   | 66000        |\n",
      "| time_elapsed       | 1.01e+03     |\n",
      "| total_timesteps    | 1320000      |\n",
      "| value_loss         | 0.023552012  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| approxkl           | 0.00813269    |\n",
      "| clipfrac           | 0.10877706    |\n",
      "| ep_len_mean        | 834           |\n",
      "| ep_reward_mean     | 0.974         |\n",
      "| explained_variance | 0.635         |\n",
      "| fps                | 1165          |\n",
      "| n_updates          | 12            |\n",
      "| policy_entropy     | 12.842502     |\n",
      "| policy_loss        | -0.0052628443 |\n",
      "| serial_timesteps   | 72000         |\n",
      "| time_elapsed       | 1.13e+03      |\n",
      "| total_timesteps    | 1440000       |\n",
      "| value_loss         | 0.021648433   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| approxkl           | 0.0078062215  |\n",
      "| clipfrac           | 0.103785396   |\n",
      "| ep_len_mean        | 834           |\n",
      "| ep_reward_mean     | 1.03          |\n",
      "| explained_variance | 0.652         |\n",
      "| fps                | 1168          |\n",
      "| n_updates          | 13            |\n",
      "| policy_entropy     | 12.833766     |\n",
      "| policy_loss        | -0.0053409464 |\n",
      "| serial_timesteps   | 78000         |\n",
      "| time_elapsed       | 1.23e+03      |\n",
      "| total_timesteps    | 1560000       |\n",
      "| value_loss         | 0.018685533   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| approxkl           | 0.007838869  |\n",
      "| clipfrac           | 0.10491457   |\n",
      "| ep_len_mean        | 834          |\n",
      "| ep_reward_mean     | 1.06         |\n",
      "| explained_variance | 0.654        |\n",
      "| fps                | 1260         |\n",
      "| n_updates          | 14           |\n",
      "| policy_entropy     | 12.815549    |\n",
      "| policy_loss        | -0.005345448 |\n",
      "| serial_timesteps   | 84000        |\n",
      "| time_elapsed       | 1.33e+03     |\n",
      "| total_timesteps    | 1680000      |\n",
      "| value_loss         | 0.017611442  |\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model_num = 0\n",
    "task = \"pushing\"\n",
    "output_path = \"experiments-pushing\"\n",
    "output_path = os.path.join(output_path, str(model_num))\n",
    "try:\n",
    "    os.makedirs(output_path)\n",
    "except FileExistsError:\n",
    "    print(\"Folder '{}' already exists. Will try to load existing checkpoints\".format(output_path))\n",
    "\n",
    "model_settings = baseline_model(model_num, task)\n",
    "\n",
    "model = utils_baselines.train_model(model_settings, output_path, tensorboard_logging)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 ('causal_world')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "14dc9ff527c7255c22ab1b01300b8a4e43cdfe49563da012c0b196015f955bf1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
